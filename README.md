# Weather Streaming Pipeline

A containerized real-time data pipeline using Kafka and Apache Spark Structured Streaming for ingesting and persisting weather data.

## What It Does

- Simulates weather sensor data as JSON and publishes it to a Kafka topic.
- Spark Structured Streaming consumes this Kafka topic in real-time.
- Spark writes the streamed data as JSON files to a local directory.
- Provides a foundation for building streaming analytics pipelines.

## Components

- **Kafka & Zookeeper:** Kafka cluster with one broker and Zookeeper for coordination.
- **Weather Producer:** Python service simulating weather data, producing to Kafka.
- **Spark Cluster:** Spark master and workers running a streaming job consuming Kafka data.
- **Spark Streaming Job:** Reads from Kafka, processes, and writes output files locally.

## How To Run

1. Clone this repo  
2. Run `docker-compose up --build`  
3. Producer publishes data to Kafka topic `weather-data`  
4. Spark job consumes and outputs JSON files to `./data/output/`  
5. Monitor Spark UI at `localhost:8080`

## Project Structure

```plaintext
producer/              # Producer code and Dockerfile
spark/                 # Spark job code and Dockerfile
data/output/           # Output JSON files generated by Spark
docker-compose.yml     # Docker Compose config for all services
.env                   # Environment variables for Kafka & Producer
README.md
.gitignore
```

## Customization

- Modify ```weather_stream_processor.py``` to add Spark transformations or analytics.
- Change producer settings to simulate different data rates or payloads.
- Scale Spark workers or Kafka partitions by updating ```docker-compose.yml```.

## Troubleshooting

- Check container logs for errors: ```docker-compose logs spark-job```
- Ensure Kafka and Zookeeper are healthy before starting producer and Spark.
- Verify volumes are properly mounted for output files.